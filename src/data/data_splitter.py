'''
data_splitter.py
~~~~~~~~~~
'''
import os
import pandas as pd
import pickle
import json
import hashlib
import logging
from scipy.sparse import load_npz, save_npz
from sklearn.model_selection import train_test_split
import numpy as np
from collections import Counter
from datetime import datetime

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def load_data():
    """
    Load the preprocessed data from disk for train/validation/test splitting.
    
    This function loads data generated by the preprocessing pipeline, including:
    1. Sample metadata with geographic origin information
    2. Sparse feature matrix of genomic variants
    3. Class labels for geographic classification
    4. Sample identifiers for traceability
    5. Label encoder for consistent class mapping
    
    Returns:
    --------
    metadata : DataFrame
        Sample metadata containing geographic and quality information
    X : sparse matrix
        Feature matrix of shape [n_samples, n_features] containing TF-IDF transformed variant data
    y : array
        Encoded geographic labels as integer indices
    sample_ids : array
        Sample IDs for linking back to original data sources
    encoder : LabelEncoder
        Label encoder for mapping between geographic names and numeric indices
        
    Design choice:
        Loading data from disk allows the splitting process to be decoupled from preprocessing,
        enabling iterative experimentation with different split strategies without
        repeating the computationally expensive preprocessing step.
    """
    metadata = pd.read_csv("data/processed/filtered_metadata.csv")
    sample_ids = metadata["Sample"].values
    
    # Set Sample as index for faster lookups - but drop=True to avoid duplicate columns
    metadata.set_index("Sample", inplace=True, drop=True)
    y = metadata["encoded_country"].values
    
    # Load sparse matrix with validation
    X = load_npz("data/processed/variant_features.npz")
    if X.shape[0] != len(metadata):
        raise ValueError(f"Mismatch between features ({X.shape[0]}) and metadata ({len(metadata)})")
    
    with open("data/processed/label_encoder.pkl", "rb") as f:
        encoder = pickle.load(f)

    return metadata, X, y, sample_ids, encoder

def split_data(features, labels, sample_ids, test_size=0.15, val_size=0.15, random_state=42, stratify=True):
    """
    Split data into training, validation, and test sets with stratification.
    
    This is a crucial function that ensures proper dataset partitioning for model training
    and evaluation. It implements a two-step splitting strategy:
    1. First split: Separate test set from the combined train+val set
    2. Second split: Separate the remaining data into training and validation sets
    
    Stratification ensures that the class distribution is preserved across all splits,
    which is essential for imbalanced genomic datasets where some geographic regions
    may have fewer samples than others.
    
    Parameters:
    -----------
    features : array-like
        Feature matrix (k-mer frequencies or variant features) of shape [n_samples, n_features]
    labels : array-like
        Geographic labels for each sample
    sample_ids : array-like
        Sample IDs for each sample to maintain traceability
    test_size : float
        Proportion of data for testing (default: 0.15 or 15%)
    val_size : float
        Proportion of data for validation (default: 0.15 or 15%)
    random_state : int
        Random seed for reproducibility (default: 42)
    stratify : bool
        Whether to perform stratified splitting to maintain class distribution (default: True)
        
    Returns:
    --------
    tuple of arrays:
        (X_train, X_val, X_test, y_train, y_val, y_test, train_indices, val_indices, test_indices)
        - Feature matrices for train, validation, and test sets
        - Labels for train, validation, and test sets
        - Indices of samples for each split for traceability
        
    Design choices:
        - Two-step splitting ensures proper stratification in all partitions
        - Using a fixed random_state ensures reproducibility
        - Preserving sample indices allows for result traceability
        - Stratification maintains class distribution despite geographic imbalances
    """
    if stratify:
        stratify_labels = labels
        logging.info("Using stratified splitting to maintain class distribution")
    else:
        stratify_labels = None
        logging.info("Using random splitting without stratification")
    
    # First split: separate test set
    X_temp, X_test, y_temp, y_test, idx_temp, idx_test = train_test_split(
        features, labels, np.arange(len(labels)),
        test_size=test_size, 
        random_state=random_state,
        stratify=stratify_labels
    )
    
    # Adjust stratification for second split
    if stratify:
        stratify_labels = y_temp
    
    # Second split: separate train and validation from the remaining data
    # Adjust validation size to account for the already removed test set
    adjusted_val_size = val_size / (1 - test_size)
    
    X_train, X_val, y_train, y_val, idx_train, idx_val = train_test_split(
        X_temp, y_temp, idx_temp,
        test_size=adjusted_val_size, 
        random_state=random_state,
        stratify=stratify_labels
    )
    
    # Get the original sample IDs
    train_ids = sample_ids[idx_train]
    val_ids = sample_ids[idx_val]
    test_ids = sample_ids[idx_test]
    
    # Log split sizes
    logging.info(f"Split sizes: Train={len(X_train)} ({len(X_train)/len(labels):.1%}), "
                 f"Val={len(X_val)} ({len(X_val)/len(labels):.1%}), "
                 f"Test={len(X_test)} ({len(X_test)/len(labels):.1%})")
    
    return X_train, X_val, X_test, y_train, y_val, y_test, idx_train, idx_val, idx_test

def check_split_feasibility(labels, test_size, val_size, min_samples_per_class=5, random_state=42):
    """
    Check if the proposed split is feasible given class distribution constraints.
    
    This function verifies that all classes will have sufficient representation in
    each data split (train, validation, test) based on the minimum samples requirement.
    It's a critical check to prevent empty classes or severely underrepresented
    categories in any partition, which could lead to training or evaluation issues.
    
    Parameters:
    -----------
    labels : array-like
        Class labels for all samples
    test_size : float
        Proportion of data for testing
    val_size : float
        Proportion of data for validation
    min_samples_per_class : int
        Minimum number of samples required for each class in each split (default: 5)
    random_state : int
        Random seed for reproducibility (default: 42)
        
    Returns:
    --------
    feasible : bool
        True if the split is feasible, False otherwise
        
    Raises:
    -------
    ValueError
        If the split is not feasible, with details about problematic classes
        
    Design choice:
        Pre-checking split feasibility prevents downstream issues during model training,
        such as empty classes in specific partitions or insufficient samples for evaluation.
        This is especially important for genomic data where class imbalance is common.
    """
    # Count samples per class
    class_counts = Counter(labels)
    
    # Calculate minimum required samples per class based on split proportions
    train_size = 1 - test_size - val_size
    
    min_required = {
        'train': max(min_samples_per_class, int(np.ceil(min_samples_per_class / train_size))),
        'val': max(min_samples_per_class, int(np.ceil(min_samples_per_class / val_size))),
        'test': max(min_samples_per_class, int(np.ceil(min_samples_per_class / test_size)))
    }
    
    total_min_required = sum(min_required.values())
    
    # Check each class
    problematic_classes = []
    for class_label, count in class_counts.items():
        if count < total_min_required:
            problematic_classes.append((class_label, count, total_min_required))
    
    # If there are problematic classes, raise an error with details
    if problematic_classes:
        error_msg = "The following classes have insufficient samples for the requested split:\n"
        for class_label, count, required in problematic_classes:
            error_msg += f"  - Class {class_label}: has {count} samples, needs {required}\n"
        error_msg += (f"Consider reducing min_samples_per_class (currently {min_samples_per_class}), "
                      f"or adjust split proportions (currently test={test_size}, val={val_size})")
        raise ValueError(error_msg)
    
    return True

def save_splits(output_dir, X_train, X_val, X_test, y_train, y_val, y_test, 
               idx_train=None, idx_val=None, idx_test=None, metadata=None, encoder=None):
    """
    Save train, validation, and test splits to disk for later use.
    
    This function persists all split data to disk in optimized formats:
    1. Feature matrices as sparse NPZ files to save storage space
    2. Labels as NumPy arrays for efficient loading
    3. Metadata as CSV files for human readability
    4. Split information as JSON for reproducibility and traceability
    
    Parameters:
    -----------
    output_dir : str
        Directory where splits will be saved
    X_train, X_val, X_test : scipy.sparse matrices
        Feature matrices for each split
    y_train, y_val, y_test : arrays
        Label arrays for each split
    idx_train, idx_val, idx_test : arrays, optional
        Indices of samples in each split for traceability
    metadata : DataFrame, optional
        Sample metadata for all samples
    encoder : LabelEncoder, optional
        Label encoder for consistent class mapping
        
    Returns:
    --------
    None
        All data is saved to disk in the specified output directory
        
    Design choices:
        - Using sparse matrices dramatically reduces storage requirements for genomic data
        - Saving indices allows for traceability back to the original dataset
        - Storing metadata facilitates debugging and analysis
        - Including timestamp and fingerprint enables reproducibility
    """
    try:
        os.makedirs(output_dir, exist_ok=True)
        
        # Save feature matrices (sparse format for efficiency)
        logging.info("Saving feature matrices...")
        save_npz(os.path.join(output_dir, "X_train.npz"), X_train)
        save_npz(os.path.join(output_dir, "X_val.npz"), X_val)
        save_npz(os.path.join(output_dir, "X_test.npz"), X_test)
        
        # Save labels
        logging.info("Saving labels...")
        np.save(os.path.join(output_dir, "train_labels.npy"), y_train)
        np.save(os.path.join(output_dir, "val_labels.npy"), y_val)
        np.save(os.path.join(output_dir, "test_labels.npy"), y_test)
        
        # Save indices if provided
        if idx_train is not None and idx_val is not None and idx_test is not None:
            logging.info("Saving indices...")
            np.save(os.path.join(output_dir, "train_indices.npy"), idx_train)
            np.save(os.path.join(output_dir, "val_indices.npy"), idx_val)
            np.save(os.path.join(output_dir, "test_indices.npy"), idx_test)
        
        # Save metadata by split if provided
        if metadata is not None:
            logging.info("Saving metadata...")
            # Reset index to get Sample column back
            metadata_with_sample = metadata.reset_index()
            
            if idx_train is not None:
                train_metadata = metadata_with_sample.iloc[idx_train].copy()
                train_metadata.to_csv(os.path.join(output_dir, "train_metadata.csv"), index=False)
            
            if idx_val is not None:
                val_metadata = metadata_with_sample.iloc[idx_val].copy()
                val_metadata.to_csv(os.path.join(output_dir, "val_metadata.csv"), index=False)
            
            if idx_test is not None:
                test_metadata = metadata_with_sample.iloc[idx_test].copy()
                test_metadata.to_csv(os.path.join(output_dir, "test_metadata.csv"), index=False)
        
        # Save encoder if provided
        if encoder is not None:
            logging.info("Saving label encoder...")
            with open(os.path.join(output_dir, "label_encoder.pkl"), "wb") as f:
                pickle.dump(encoder, f)
        
        # Save split information as JSON
        logging.info("Saving split information...")
        split_info = {
            "train_size": len(X_train) / (len(X_train) + len(X_val) + len(X_test)),
            "val_size": len(X_val) / (len(X_train) + len(X_val) + len(X_test)),
            "test_size": len(X_test) / (len(X_train) + len(X_val) + len(X_test)),
            "n_features": X_train.shape[1],
            "n_classes": len(np.unique(np.concatenate([y_train, y_val, y_test]))),
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "class_distribution": {
                "train": dict(Counter(y_train)),
                "val": dict(Counter(y_val)),
                "test": dict(Counter(y_test))
            }
        }
        
        # Add data fingerprint for tracking
        if metadata is not None:
            metadata_hash = hashlib.md5(pd.util.hash_pandas_object(metadata).values).hexdigest()
            feature_info = f"{X_train.shape}|{X_train.nnz}|{X_val.shape}|{X_val.nnz}|{X_test.shape}|{X_test.nnz}"
            split_info["data_fingerprint"] = hashlib.md5((metadata_hash + feature_info).encode()).hexdigest()
        
        with open(os.path.join(output_dir, "split_info.json"), "w") as f:
            json.dump(split_info, f, indent=2)
        
        logging.info(f"All splits saved to {output_dir}")
    
    except Exception as e:
        logging.error(f"Error saving splits: {e}")
        raise

def load_splits(input_dir):
    """
    Load previously saved data splits from disk.
    
    This function loads all components of a dataset split from a specified directory,
    reconstructing the training, validation, and test sets for model training or evaluation.
    It's the counterpart to the save_splits function and ensures all data is loaded
    in a consistent format.
    
    Parameters:
    -----------
    input_dir : str
        Directory containing the saved splits
        
    Returns:
    --------
    dict
        Dictionary containing all loaded components:
        - X_train, X_val, X_test: Feature matrices
        - y_train, y_val, y_test: Labels
        - train_metadata, val_metadata, test_metadata: Sample metadata if available
        - encoder: Label encoder if available
        - split_info: Split information and statistics
        
    Design choice:
        Loading all components in a single function ensures consistency and simplifies
        the API for downstream code. The dictionary return format provides flexibility
        for partial loading if needed.
    """
    result = {}
    
    logging.info(f"Loading splits from {input_dir}...")
    
    # Load feature matrices
    if os.path.exists(os.path.join(input_dir, "X_train.npz")):
        result["X_train"] = load_npz(os.path.join(input_dir, "X_train.npz"))
        logging.info(f"Loaded training features: {result['X_train'].shape}")
    
    if os.path.exists(os.path.join(input_dir, "X_val.npz")):
        result["X_val"] = load_npz(os.path.join(input_dir, "X_val.npz"))
        logging.info(f"Loaded validation features: {result['X_val'].shape}")
    
    if os.path.exists(os.path.join(input_dir, "X_test.npz")):
        result["X_test"] = load_npz(os.path.join(input_dir, "X_test.npz"))
        logging.info(f"Loaded test features: {result['X_test'].shape}")
    
    # Load labels - try both .npz and .npy formats for compatibility
    for split in ["train", "val", "test"]:
        label_key = f"y_{split}"
        npy_path = os.path.join(input_dir, f"{split}_labels.npy")
        npz_path = os.path.join(input_dir, f"{split}_labels.npz")
        
        if os.path.exists(npy_path):
            result[label_key] = np.load(npy_path)
            logging.info(f"Loaded {split} labels: {len(result[label_key])}")
        elif os.path.exists(npz_path):
            result[label_key] = np.load(npz_path)["labels"]
            logging.info(f"Loaded {split} labels (npz): {len(result[label_key])}")
    
    # Load metadata if available
    for split in ["train", "val", "test"]:
        metadata_path = os.path.join(input_dir, f"{split}_metadata.csv")
        if os.path.exists(metadata_path):
            result[f"{split}_metadata"] = pd.read_csv(metadata_path)
            logging.info(f"Loaded {split} metadata: {len(result[f'{split}_metadata'])} rows")
    
    # Load label encoder if available
    encoder_path = os.path.join(input_dir, "label_encoder.pkl")
    if os.path.exists(encoder_path):
        with open(encoder_path, "rb") as f:
            result["encoder"] = pickle.load(f)
        logging.info(f"Loaded label encoder with {len(result['encoder'].classes_)} classes")
    
    # Load split info if available
    split_info_path = os.path.join(input_dir, "split_info.json")
    if os.path.exists(split_info_path):
        with open(split_info_path, "r") as f:
            result["split_info"] = json.load(f)
    
    return result

def main():
    """
    Main function to split and save the preprocessed data.
    
    This function orchestrates the entire data splitting workflow:
    1. Load preprocessed data from disk
    2. Verify that splitting is feasible with class distribution constraints
    3. Split data into training, validation, and test sets
    4. Save all splits to disk for downstream model training
    
    The function includes optimization to only create a new split if one with 
    the requested proportions doesn't already exist, which saves computational
    resources during iterative development.
    
    Returns:
    --------
    None
        Results are saved to disk in the specified output directory
        
    Design choices:
        - Reusing existing splits when possible avoids redundant processing
        - Data fingerprinting ensures consistency across preprocessing changes
        - Stratified splitting maintains class distribution despite imbalances
        - Comprehensive logging provides transparency and debugging information
    """
    # Load data
    metadata, X, y, sample_ids, encoder = load_data()
    
    # Define split proportions
    test_size = 0.15
    val_size = 0.15
    train_size = 1 - test_size - val_size
    random_state = 42
    
    # Validate split proportions
    if test_size + val_size >= 1.0:
        raise ValueError("The sum of test_size and val_size must be less than 1.0.")
    
    # Create output directory
    split_dir = os.path.join("data", "split")
    try:
        os.makedirs(split_dir, exist_ok=True)
    except OSError as e:
        logging.error(f"Failed to create directory {split_dir}: {e}")
        return
    
    # Generate improved data fingerprint
    metadata_hash = hashlib.md5(pd.util.hash_pandas_object(metadata).values).hexdigest()
    feature_info = f"{X.shape}|{X.nnz}"
    data_fingerprint = hashlib.md5((metadata_hash + feature_info).encode()).hexdigest()
    
    # Check if split already exists with requested proportions
    split_info_path = os.path.join(split_dir, "split_info.json")
    if os.path.exists(split_info_path):
        try:
            with open(split_info_path, "r") as f:
                existing_info = json.load(f)
            
            # Check data fingerprint first to detect data changes
            if existing_info.get("data_fingerprint") == data_fingerprint:
                logging.info(f"Found existing split with same data fingerprint")
                logging.info(f"Using existing split in {split_dir}")
                return
                
            # Otherwise check proportions
            existing_train = existing_info.get("train_size")
            existing_val = existing_info.get("val_size")
            existing_test = existing_info.get("test_size")
            
            if not all(isinstance(x, (int, float)) for x in [existing_train, existing_val, existing_test]):
                raise ValueError("Split proportions must be numeric.")
            
            # Check if existing split has same proportions (with small tolerance for floating point differences)
            if (abs(existing_train - train_size) < 0.01 and 
                abs(existing_val - val_size) < 0.01 and
                abs(existing_test - test_size) < 0.01):
                logging.info(f"Found existing split with {existing_train:.0%}/{existing_val:.0%}/{existing_test:.0%} train/val/test ratio")
                logging.info(f"Data has changed, re-creating the split with same proportions")
        except (json.JSONDecodeError, ValueError) as e:
            logging.warning(f"Failed to read or validate split_info.json: {e}")
            logging.info("Proceeding to create a new split.")
        except IOError as e:
            logging.error(f"Failed to open or read split_info.json: {e}")
            return
    
    logging.info(f"Loading data from processed directory...")
    logging.info(f"Found {len(metadata)} samples with {X.shape[1]} features")
    logging.info(f"Countries represented: {len(encoder.classes_)}")
    
    # Check if split is feasible with the same random_state as actual split
    try:
        check_split_feasibility(y, test_size=test_size, val_size=val_size, 
                               min_samples_per_class=5, random_state=random_state)
    except ValueError as e:
        logging.error(f"Error: {e}")
        logging.error("Data split is not feasible with the current configuration.")
        return
    
    # Perform the split
    logging.info(f"Splitting data with {train_size:.0%}/{val_size:.0%}/{test_size:.0%} train/val/test ratio...")
    X_train, X_val, X_test, y_train, y_val, y_test, idx_train, idx_val, idx_test = split_data(
        X, y, sample_ids, test_size=test_size, val_size=val_size, random_state=random_state, stratify=True
    )
    
    # Verify split sizes
    if X_train.shape[0] == 0 or X_val.shape[0] == 0 or X_test.shape[0] == 0:
        logging.error("One or more splits are empty. Check your split proportions.")
        return
    
    # Report split sizes
    logging.info(f"Split sizes:")
    logging.info(f"  Train: {X_train.shape[0]} samples ({X_train.shape[0]/len(X):.1%})")
    logging.info(f"  Validation: {X_val.shape[0]} samples ({X_val.shape[0]/len(X):.1%})")
    logging.info(f"  Test: {X_test.shape[0]} samples ({X_test.shape[0]/len(X):.1%})")
    
    # Save all split data with additional metadata
    logging.info(f"Saving split data to {split_dir}...")
    try:
        save_splits(
            output_dir=split_dir, 
            X_train=X_train, X_val=X_val, X_test=X_test, 
            y_train=y_train, y_val=y_val, y_test=y_test,
            idx_train=idx_train, idx_val=idx_val, idx_test=idx_test,
            metadata=metadata, 
            encoder=encoder
        )
    except IOError as e:
        logging.error(f"Failed to save split data: {e}")
        return
    
    logging.info(f"Data splitting complete. Files saved to {split_dir}")
    logging.info(f"You can now proceed to model training using the split data.")

if __name__ == "__main__":
    main()