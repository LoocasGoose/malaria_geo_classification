{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Malaria Geographic Origin Model Comparison\n",
    "\n",
    "This notebook compares the performance of different models for classifying malaria parasites by their geographic origin:\n",
    "1. **Multinomial Naive Bayes**: A classical machine learning approach\n",
    "2. **Standard CNN**: A basic convolutional neural network\n",
    "3. **Advanced CNN with Strand Symmetry**: Enhanced CNN with reverse complement equivariance\n",
    "4. **Advanced CNN without Strand Symmetry**: Enhanced CNN without reverse complement equivariance\n",
    "\n",
    "A key focus of this analysis is understanding the impact of reverse complement equivariance (strand symmetry) on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "\n",
    "# Create directories for outputs\n",
    "os.makedirs('../reports/figures', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Project Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import project modules\n",
    "from src.models.cnn_standard import DNACNN as StandardCNN  \n",
    "from src.models.cnn_advanced import DNACNN as AdvancedCNN\n",
    "from src.models.naive_bayes import MultinomialNaiveBayes\n",
    "from src.data.genomic_sequences import GenomicSequenceDataset\n",
    "from src.evaluation.model_evaluator import evaluate_model_detailed\n",
    "from src.evaluation.model_comparison import (\n",
    "    compare_models, \n",
    "    calculate_roc_curves, \n",
    "    analyze_complexity_tradeoff,\n",
    "    analyze_per_class_performance,\n",
    "    identify_challenging_classes,\n",
    "    identify_best_model_per_class,\n",
    "    calculate_model_size\n",
    ")\n",
    "from src.evaluation.compare_strand_symmetry import (\n",
    "    create_strand_symmetric_model,\n",
    "    evaluate_strand_symmetry_effect,\n",
    "    plot_strand_symmetry_comparison,\n",
    "    run_strand_symmetry_analysis\n",
    ")\n",
    "from src.visualization.performance_visualizer import (\n",
    "    plot_confusion_matrix, \n",
    "    plot_roc_curves, \n",
    "    plot_metrics_comparison, \n",
    "    plot_complexity_tradeoff,\n",
    "    plot_per_class_performance,\n",
    "    plot_training_history,\n",
    "    plot_model_architecture_comparison,\n",
    "    plot_challenging_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_dataset = GenomicSequenceDataset(\n",
    "    split_dir=\"../data/split\",\n",
    "    split_type=\"test\",\n",
    "    window_size=1000,\n",
    "    stride=500,\n",
    "    cache_size=128\n",
    ")\n",
    "\n",
    "test_loader = test_dataset.get_dataloader(batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "# Define class names\n",
    "class_names = test_dataset.encoder.classes_\n",
    "print(f\"Number of classes: {len(class_names)}\")\n",
    "print(f\"Class names: {class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load CNN models\n",
    "try:\n",
    "    standard_cnn = torch.load('../models/cnn_standard_best.pt', map_location=device)\n",
    "    print(\"Loaded standard CNN model\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Standard CNN model not found. Please train the model first.\")\n",
    "    standard_cnn = None\n",
    "\n",
    "# Load Advanced CNN with strand symmetry\n",
    "try:\n",
    "    advanced_cnn_symmetric = torch.load('../models/cnn_advanced_symmetric.pt', map_location=device)\n",
    "    print(\"Loaded advanced CNN model with strand symmetry\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Advanced CNN model with strand symmetry not found. Please train the model first.\")\n",
    "    advanced_cnn_symmetric = None\n",
    "\n",
    "# Load Advanced CNN without strand symmetry\n",
    "try:\n",
    "    advanced_cnn_standard = torch.load('../models/cnn_advanced_standard.pt', map_location=device)\n",
    "    print(\"Loaded advanced CNN model without strand symmetry\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Advanced CNN model without strand symmetry not found. Please train the model first.\")\n",
    "    advanced_cnn_standard = None\n",
    "    \n",
    "# Load Naive Bayes model\n",
    "try:\n",
    "    naive_bayes = MultinomialNaiveBayes.load('../models/naive_bayes_model.pkl')\n",
    "    print(\"Loaded Naive Bayes model\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Naive Bayes model not found. Please train the model first.\")\n",
    "    naive_bayes = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate CNN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function for deep models\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# List to store all model results\n",
    "all_model_results = []\n",
    "\n",
    "# Evaluate Standard CNN\n",
    "if standard_cnn is not None:\n",
    "    print(\"Evaluating Standard CNN...\")\n",
    "    standard_cnn_results = evaluate_model_detailed(\n",
    "        standard_cnn, test_loader, criterion, device, model_name='CNN (Standard)'\n",
    "    )\n",
    "    all_model_results.append(standard_cnn_results)\n",
    "    print(f\"Standard CNN Accuracy: {standard_cnn_results['accuracy']:.4f}\")\n",
    "\n",
    "# Evaluate Advanced CNN with strand symmetry\n",
    "if advanced_cnn_symmetric is not None:\n",
    "    print(\"\\nEvaluating Advanced CNN with Strand Symmetry...\")\n",
    "    advanced_symmetric_results = evaluate_model_detailed(\n",
    "        advanced_cnn_symmetric, test_loader, criterion, device, model_name='CNN (Advanced, Symmetric)'\n",
    "    )\n",
    "    all_model_results.append(advanced_symmetric_results)\n",
    "    print(f\"Advanced CNN (Symmetric) Accuracy: {advanced_symmetric_results['accuracy']:.4f}\")\n",
    "\n",
    "# Evaluate Advanced CNN without strand symmetry\n",
    "if advanced_cnn_standard is not None:\n",
    "    print(\"\\nEvaluating Advanced CNN without Strand Symmetry...\")\n",
    "    advanced_standard_results = evaluate_model_detailed(\n",
    "        advanced_cnn_standard, test_loader, criterion, device, model_name='CNN (Advanced, Standard)'\n",
    "    )\n",
    "    all_model_results.append(advanced_standard_results)\n",
    "    print(f\"Advanced CNN (Standard) Accuracy: {advanced_standard_results['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Naive Bayes, we need a different approach as it doesn't use PyTorch\n",
    "if naive_bayes is not None:\n",
    "    print(\"\\nEvaluating Naive Bayes...\")\n",
    "    \n",
    "    # Extract test data in a format suitable for Naive Bayes\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    \n",
    "    for batch in test_loader:\n",
    "        sequences = batch['sequence'].cpu().numpy()\n",
    "        labels = batch['label'].cpu().numpy()\n",
    "        \n",
    "        # Flatten and convert each sequence for Naive Bayes\n",
    "        for i in range(len(sequences)):\n",
    "            X_test.append(sequences[i].flatten())\n",
    "            y_test.append(labels[i])\n",
    "\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    # Time the Naive Bayes inference\n",
    "    start_time = time.time()\n",
    "    nb_predictions = naive_bayes.predict(X_test)\n",
    "    nb_inference_time = (time.time() - start_time) / len(X_test)\n",
    "    \n",
    "    # Calculate metrics for Naive Bayes\n",
    "    nb_accuracy = accuracy_score(y_test, nb_predictions)\n",
    "    nb_precision = precision_score(y_test, nb_predictions, average='weighted', zero_division=0)\n",
    "    nb_recall = recall_score(y_test, nb_predictions, average='weighted', zero_division=0)\n",
    "    nb_f1 = f1_score(y_test, nb_predictions, average='weighted', zero_division=0)\n",
    "    nb_class_report = classification_report(y_test, nb_predictions, output_dict=True, zero_division=0)\n",
    "    nb_conf_matrix = confusion_matrix(y_test, nb_predictions)\n",
    "    \n",
    "    # Try to get probabilities if the model supports it\n",
    "    try:\n",
    "        nb_probabilities = naive_bayes.predict_proba(X_test)\n",
    "    except:\n",
    "        nb_probabilities = None\n",
    "    \n",
    "    # Store Naive Bayes results\n",
    "    naive_bayes_results = {\n",
    "        'model_name': 'Naive Bayes',\n",
    "        'accuracy': nb_accuracy,\n",
    "        'precision': nb_precision,\n",
    "        'recall': nb_recall,\n",
    "        'f1_score': nb_f1,\n",
    "        'test_loss': 0.0,  # Naive Bayes doesn't have a loss\n",
    "        'class_report': nb_class_report,\n",
    "        'confusion_matrix': nb_conf_matrix,\n",
    "        'predictions': nb_predictions,\n",
    "        'true_labels': y_test,\n",
    "        'probabilities': nb_probabilities,\n",
    "        'avg_inference_time': nb_inference_time,\n",
    "        'inference_times': [nb_inference_time] * len(y_test)  # Approximation\n",
    "    }\n",
    "    \n",
    "    all_model_results.append(naive_bayes_results)\n",
    "    print(f\"Naive Bayes Accuracy: {nb_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "if len(all_model_results) > 0:\n",
    "    comparison_results = compare_models(all_model_results)\n",
    "    \n",
    "    # Display metrics table\n",
    "    metrics_df = comparison_results['metrics_table']\n",
    "    metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Strand Symmetry Effect\n",
    "\n",
    "This section specifically focuses on comparing the impact of reverse complement equivariance (strand symmetry) on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run strand symmetry analysis using dedicated function\n",
    "symmetry_results = run_strand_symmetry_analysis(test_loader, device)\n",
    "\n",
    "# Display results\n",
    "if len(symmetry_results) >= 2:\n",
    "    sym_model = list(symmetry_results.keys())[0]  # First key (symmetric model)\n",
    "    std_model = list(symmetry_results.keys())[1]  # Second key (standard model)\n",
    "    \n",
    "    # Create comparison table\n",
    "    sym_comparison = pd.DataFrame({\n",
    "        'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "        sym_model: [\n",
    "            symmetry_results[sym_model]['accuracy'],\n",
    "            symmetry_results[sym_model]['precision'],\n",
    "            symmetry_results[sym_model]['recall'],\n",
    "            symmetry_results[sym_model]['f1_score']\n",
    "        ],\n",
    "        std_model: [\n",
    "            symmetry_results[std_model]['accuracy'],\n",
    "            symmetry_results[std_model]['precision'],\n",
    "            symmetry_results[std_model]['recall'],\n",
    "            symmetry_results[std_model]['f1_score']\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Calculate improvement percentage\n",
    "    sym_comparison['Improvement (%)'] = ((sym_comparison[sym_model] - sym_comparison[std_model]) / sym_comparison[std_model] * 100).round(2)\n",
    "    \n",
    "    # Display table\n",
    "    sym_comparison\n",
    "else:\n",
    "    print(\"Not enough models to compare strand symmetry effect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Class-Specific Strand Symmetry Impact\n",
    "\n",
    "Let's examine whether strand symmetry has a larger impact on certain geographic regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare per-class performance between symmetry and non-symmetry models\n",
    "if len(symmetry_results) >= 2:\n",
    "    # Get per-class F1 scores from both models\n",
    "    per_class_sym = []\n",
    "    \n",
    "    for model_name, result in symmetry_results.items():\n",
    "        for class_idx, metrics in result['class_report'].items():\n",
    "            if class_idx in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "                continue\n",
    "                \n",
    "            if class_names is not None and int(class_idx) < len(class_names):\n",
    "                class_label = class_names[int(class_idx)]\n",
    "            else:\n",
    "                class_label = f\"Class {class_idx}\"\n",
    "                \n",
    "            per_class_sym.append({\n",
    "                'Model': model_name,\n",
    "                'Class': class_label,\n",
    "                'F1 Score': metrics['f1-score']\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    per_class_df = pd.DataFrame(per_class_sym)\n",
    "    \n",
    "    # Pivot to wide format for comparison\n",
    "    pivot_df = per_class_df.pivot(index='Class', columns='Model', values='F1 Score')\n",
    "    \n",
    "    # Calculate improvement percentage\n",
    "    sym_model = list(symmetry_results.keys())[0]  # Symmetric model\n",
    "    std_model = list(symmetry_results.keys())[1]  # Standard model\n",
    "    pivot_df['Improvement (%)'] = ((pivot_df[sym_model] - pivot_df[std_model]) / pivot_df[std_model] * 100).round(2)\n",
    "    \n",
    "    # Sort by improvement percentage\n",
    "    pivot_df = pivot_df.sort_values('Improvement (%)', ascending=False)\n",
    "    \n",
    "    # Display top 10 most improved classes\n",
    "    print(\"Classes with largest improvement from strand symmetry:\")\n",
    "    pivot_df.head(10)\n",
    "else:\n",
    "    print(\"Not enough models to compare per-class strand symmetry effect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Strand Symmetry Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot strand symmetry comparison\n",
    "if len(symmetry_results) >= 2:\n",
    "    fig = plot_strand_symmetry_comparison(symmetry_results)\n",
    "    plt.show()\n",
    "    \n",
    "    # Save figure\n",
    "    fig.savefig('../reports/figures/strand_symmetry_effect.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Improvement Percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display performance improvement\n",
    "if len(all_model_results) > 1:\n",
    "    improvement_metrics = metrics_df[['Model', 'Accuracy Improvement (%)', 'F1 Score Improvement (%)']]\n",
    "    improvement_metrics = improvement_metrics.dropna().round(2)\n",
    "    improvement_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices\n",
    "for result in all_model_results:\n",
    "    fig = plot_confusion_matrix(\n",
    "        result['true_labels'], \n",
    "        result['predictions'],\n",
    "        class_names=class_names,\n",
    "        title=f'{result[\"model_name\"]} Confusion Matrix'\n",
    "    )\n",
    "    plt.show()\n",
    "    \n",
    "    # Save figure\n",
    "    model_name = result['model_name'].replace(' ', '_').lower().replace('(', '').replace(')', '').replace(',', '')\n",
    "    fig.savefig(f'../reports/figures/{model_name}_confusion_matrix.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Complexity vs. Performance Tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze complexity tradeoff\n",
    "if len(all_model_results) > 1:\n",
    "    tradeoff_results = analyze_complexity_tradeoff(all_model_results)\n",
    "    \n",
    "    # Plot complexity tradeoff\n",
    "    fig = plot_complexity_tradeoff(tradeoff_results)\n",
    "    plt.show()\n",
    "    \n",
    "    # Save figure\n",
    "    fig.savefig('../reports/figures/complexity_tradeoff.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Performance Metrics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot metrics comparison\n",
    "if len(all_model_results) > 1:\n",
    "    fig = plot_metrics_comparison(metrics_df)\n",
    "    plt.show()\n",
    "    \n",
    "    # Save figure\n",
    "    fig.savefig('../reports/figures/metrics_comparison.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Per-Class Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze per-class performance\n",
    "if len(all_model_results) > 0:\n",
    "    per_class_results = analyze_per_class_performance(all_model_results, class_names)\n",
    "    \n",
    "    # Display per-class results\n",
    "    per_class_results.head(10)\n",
    "\n",
    "# Plot per-class F1 scores\n",
    "if len(all_model_results) > 0:\n",
    "    fig = plot_per_class_performance(per_class_results, metric='F1 Score')\n",
    "    plt.show()\n",
    "    \n",
    "    # Save figure\n",
    "    fig.savefig('../reports/figures/per_class_f1.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Best Model for Each Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate which model performs best on each class\n",
    "if len(all_model_results) > 1:\n",
    "    best_model_per_class = identify_best_model_per_class(per_class_results)\n",
    "    best_model_per_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Challenging Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify challenging classes\n",
    "if len(all_model_results) > 0:\n",
    "    challenging_classes = identify_challenging_classes(per_class_results, threshold=0.7)\n",
    "    print(f\"Challenging classes (F1 < 0.7): {challenging_classes}\")\n",
    "    \n",
    "    if challenging_classes:\n",
    "        # Plot performance on challenging classes\n",
    "        fig = plot_challenging_classes(per_class_results, challenging_classes)\n",
    "        plt.show()\n",
    "        \n",
    "        # Save figure\n",
    "        fig.savefig('../reports/figures/challenging_classes.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary of findings\n",
    "if len(all_model_results) > 0:\n",
    "    print(\"\\n=== MODEL EVALUATION SUMMARY ===\")\n",
    "    for result in all_model_results:\n",
    "        print(f\"\\n{result['model_name']}:\")\n",
    "        print(f\"  - Accuracy: {result['accuracy']:.4f}\")\n",
    "        print(f\"  - F1 Score: {result['f1_score']:.4f}\")\n",
    "        print(f\"  - Inference Time: {result['avg_inference_time']*1000:.2f} ms per sample\")\n",
    "    \n",
    "    if len(all_model_results) > 1:\n",
    "        # Find best model by accuracy\n",
    "        best_acc_idx = np.argmax([r['accuracy'] for r in all_model_results])\n",
    "        best_acc_model = all_model_results[best_acc_idx]['model_name']\n",
    "        \n",
    "        # Find fastest model\n",
    "        fastest_idx = np.argmin([r['avg_inference_time'] for r in all_model_results])\n",
    "        fastest_model = all_model_results[fastest_idx]['model_name']\n",
    "        \n",
    "        print(f\"\\nBest accuracy: {best_acc_model} ({all_model_results[best_acc_idx]['accuracy']:.4f})\")\n",
    "        print(f\"Fastest inference: {fastest_model} ({all_model_results[fastest_idx]['avg_inference_time']*1000:.2f} ms)\")\n",
    "        \n",
    "        # Print challenging classes\n",
    "        if challenging_classes:\n",
    "            print(f\"\\nChallenging classes: {', '.join(challenging_classes)}\")\n",
    "            \n",
    "        # Print summary of strand symmetry effect\n",
    "        if len(symmetry_results) >= 2:\n",
    "            sym_model = list(symmetry_results.keys())[0]  # Symmetric model\n",
    "            std_model = list(symmetry_results.keys())[1]  # Standard model\n",
    "            \n",
    "            acc_diff = symmetry_results[sym_model]['accuracy'] - symmetry_results[std_model]['accuracy']\n",
    "            acc_pct = acc_diff / symmetry_results[std_model]['accuracy'] * 100\n",
    "            \n",
    "            print(f\"\\nImpact of Strand Symmetry (Reverse Complement Equivariance):\")\n",
    "            print(f\"  - Accuracy improvement: {acc_diff*100:.2f} percentage points ({acc_pct:.2f}%)\")\n",
    "            \n",
    "            # Show top 3 most improved classes\n",
    "            if 'Improvement (%)' in pivot_df.columns:\n",
    "                top_improved = pivot_df.head(3)\n",
    "                print(f\"  - Classes with largest improvement from strand symmetry:\")\n",
    "                for idx, row in top_improved.iterrows():\n",
    "                    print(f\"    * {idx}: {row['Improvement (%)']:.2f}% improvement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Results for Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to CSV files\n",
    "if len(all_model_results) > 0:\n",
    "    # Export metrics table\n",
    "    metrics_df.to_csv('../reports/model_metrics.csv', index=False)\n",
    "    \n",
    "    # Export per-class results\n",
    "    per_class_results.to_csv('../reports/per_class_metrics.csv', index=False)\n",
    "    \n",
    "    # Export best model per class\n",
    "    if len(all_model_results) > 1:\n",
    "        best_model_per_class.to_csv('../reports/best_model_per_class.csv', index=False)\n",
    "    \n",
    "    # Export strand symmetry comparison\n",
    "    if len(symmetry_results) >= 2:\n",
    "        sym_comparison.to_csv('../reports/strand_symmetry_comparison.csv', index=False)\n",
    "        pivot_df.to_csv('../reports/strand_symmetry_per_class.csv')\n",
    "        \n",
    "    print(\"Results exported to CSV files in '../reports/' directory\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
